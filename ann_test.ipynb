{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218aeb79",
   "metadata": {},
   "source": [
    "## Predication with Different Classification Method to The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cecb0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Ensemble Methods\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df5f1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b276802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(obj, p, cycle)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set max output lines before scrolling\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.instance().display_formatter.formatters['text/plain'].for_type(\n",
    "    type, lambda obj, p, cycle: p.text(repr(obj)[:10000])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941cad",
   "metadata": {},
   "source": [
    "### Metrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aceecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric Calculations\n",
    "\n",
    "def calculate_metrics(classifier, y_val, y_pred):\n",
    "    print(f\"{classifier} metrics: \")\n",
    "\n",
    "    print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9677b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_accuracy_gen(model, label, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    calculate_metrics(label, y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a91ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationPipeline:\n",
    "\n",
    "    param_grid_logistic_regression = {\n",
    "        'C': [0.01, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    param_grid_decission_tree_classifier = {\n",
    "        'max_depth': [None, 5, 20, 50],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "\n",
    "    param_grid_random_forest_classifier = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 20, 50],\n",
    "        'bootstrap': [True, False],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    param_grid_gaussian_naive_bias = {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "    }\n",
    "\n",
    "    param_grid_svc = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'kernel': ['rbf', 'poly']\n",
    "    }\n",
    "\n",
    "    param_grid_knn = {\n",
    "        'n_neighbors': [100, 500, 700, 900, 1100, 1500],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    param_grid_ada_boost = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.5, 1.0],\n",
    "        'estimator': [\n",
    "            DecisionTreeClassifier(max_depth=1),\n",
    "            DecisionTreeClassifier(max_depth=3)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2], \n",
    "        'subsample': [0.6, 0.8, 1.0],     \n",
    "        'gamma': [0, 0.1, 0.3, 0.5],           \n",
    "    }\n",
    "\n",
    "    param_grid_ann = {\n",
    "        'model__n_neurons': [32, 64, 128],\n",
    "        'model__activation': ['relu', 'tanh'],\n",
    "        'epochs': [10, 20],\n",
    "        'batch_size': [16, 32, 64]\n",
    "    }\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.feature_path = file_path\n",
    "        self.feature_df = self.get_feture()\n",
    "        self.X, self.y = self.split_feture_and_target()\n",
    "        self.y = self.map_zero_to_n() # mapping y zero to number of class to make it usable for some modles i.e. xgaboost\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.get_scale_and_test_train_split()\n",
    "\n",
    "    # data read and processing section\n",
    "    def remove_outliear(self, feature_df):\n",
    "        iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "        outliers = iso.fit_predict(feature_df)\n",
    "        data_cleaned = feature_df[outliers == 1]\n",
    "\n",
    "        return data_cleaned\n",
    "\n",
    "    def get_feture(self):\n",
    "        feature_df = pd.read_csv(self.feature_path)\n",
    "        feature_df = feature_df.iloc[:, 1:] # remove index\n",
    "        \n",
    "        return self.remove_outliear(feature_df)\n",
    "\n",
    "    def split_feture_and_target(self):\n",
    "        X = self.feature_df.iloc[:, :-1]\n",
    "        y = self.feature_df.iloc[:, -1]\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def get_scale_and_test_train_split(self):\n",
    "        #Scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaled_fature = scaler.fit_transform(self.X)\n",
    "        \n",
    "        #test train split\n",
    "        return train_test_split(scaled_fature, self.y, train_size=.20, random_state=42, stratify=self.y)\n",
    "    \n",
    "    def map_zero_to_n(self):\n",
    "        unique_values = {val: idx for idx, val in enumerate(self.y.unique())}\n",
    "        y_mapped = self.y.map(unique_values)\n",
    "\n",
    "        return y_mapped\n",
    "    \n",
    "    # Cross validation section \n",
    "    def kfold_cross_validation(self, model, n_splits):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        kfold_score = cross_val_score(model, self.X, self.y, cv=kf)\n",
    "\n",
    "        print(\"K-fold cross validaiton scores:\", kfold_score)\n",
    "\n",
    "    def stratified_cross_validation(self, model, n_splits):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        skfold_score = cross_val_score(model, self.X, self.y, cv=skf)\n",
    "\n",
    "        print(\"Straified cross validation scores:\", skfold_score)\n",
    "\n",
    "    def cross_validation(self, model, n_splits):\n",
    "        self.kfold_cross_validation(model, n_splits)\n",
    "        self.stratified_cross_validation(model, n_splits)\n",
    "\n",
    "    # Hyper parameter tuning\n",
    "\n",
    "    def gridSerach(self, estimator, param_grid):\n",
    "        print(\"==== Grid Search: =====\")\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3, verbose=0)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Best parameters found: \", grid_search.best_params_)\n",
    "        print(\"Best score found: \", grid_search.best_score_)\n",
    "\n",
    "        return grid_search\n",
    "    \n",
    "    def randomSearch(self, estimator, param_grid):\n",
    "        print(\"\\n==== Random Search: =====\")\n",
    "\n",
    "        random_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=500, cv=3, random_state=42)\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Best parameters found: \", random_search.best_params_)\n",
    "        print(\"Best score found: \", random_search.best_score_)\n",
    "\n",
    "        return random_search\n",
    "    \n",
    "    def hyper_parameter_tuning(self, model, param_grid):\n",
    "        grid_search = self.gridSerach(model, param_grid)\n",
    "        random_search = self.randomSearch(model, param_grid)\n",
    "\n",
    "        return grid_search if grid_search.best_score_ > random_search.best_score_ else random_search\n",
    "    \n",
    "    # Models section\n",
    "    def run_logistic_regression_model(self):\n",
    "        print(\"=============== 1. Logistic Regression Section: ==================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(LogisticRegression(), self.param_grid_logistic_regression)\n",
    "        lrm = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(lrm, \"1. Logistic regression\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(lrm, 10)\n",
    "    \n",
    "    def run_decission_tree_classifier_model(self):\n",
    "        print(\"=================2. Decission Tree Classifier Section: ================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(DecisionTreeClassifier(), self.param_grid_decission_tree_classifier)\n",
    "        dt = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(dt, \"2. Decission Tree Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(dt, 10)\n",
    "\n",
    "    def run_random_forest_classifier_model(self):\n",
    "        print(\"=================== 3. Random Forest Classifier Section: ==================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(RandomForestClassifier(), self.param_grid_random_forest_classifier)\n",
    "        rfc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(rfc, \"3.  Random Forest Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(rfc, 10)\n",
    "\n",
    "    def run_gaussian_naive_bias_classifier_model(self):\n",
    "        print(\"=================== 4. Gaussian Naive Bias Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(GaussianNB(), self.param_grid_gaussian_naive_bias)\n",
    "        gnb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(gnb, \"4. Gaussian Naive Bias Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(gnb, 10)\n",
    "\n",
    "\n",
    "    def run_support_vector_classifier_model(self):\n",
    "        print(\"=================== 5. Support Vector Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(SVC(), self.param_grid_svc)\n",
    "        svc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(svc, \"5. Support Vector Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(svc, 10)\n",
    "\n",
    "\n",
    "    def run_knn_classifier_model(self):\n",
    "        print(\"=================== 6. K-Nearest Neighbors Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(KNeighborsClassifier(), self.param_grid_knn)\n",
    "        knn = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(knn, \"6. K-Nearest Neighbors\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(knn, 10)\n",
    "\n",
    "    def run_ada_boost_classifier_model(self):\n",
    "        print(\"=================== 7. Ada Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(AdaBoostClassifier(), self.param_grid_ada_boost)\n",
    "        abc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(abc, \"7. Ada Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(abc, 10)\n",
    "\n",
    "    def run_xg_boost_classifier_model(self):\n",
    "        print(\"=================== 8. XG Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(XGBClassifier(), self.param_grid_xgb)\n",
    "        xgb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(xgb, \"8. XG Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(xgb, 10)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ann(n_neurons=64, activation='relu'):\n",
    "        model = Sequential()\n",
    "        # Input layer\n",
    "        model.add(Dense(n_neurons, activation=activation, input_shape=(24,)))\n",
    "        \n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "            \n",
    "        # Output layer (example for binary classification)\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def run_ann_model(self):\n",
    "        print(\"=================== 9. Artificial Neural Net Section: ===================\")\n",
    "\n",
    "        model = KerasClassifier(build_fn=self.build_ann, verbose=0)\n",
    "        \n",
    "        tuned_model = self.hyper_parameter_tuning(model, self.param_grid_ann)\n",
    "        ann = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(ann, \"9. Artificial Neuralnet\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(ann, 10)\n",
    "\n",
    "    def driver(self):\n",
    "        self.run_logistic_regression_model()\n",
    "        self.run_decission_tree_classifier_model()\n",
    "        self.run_random_forest_classifier_model()\n",
    "        self.run_gaussian_naive_bias_classifier_model()\n",
    "        self.run_support_vector_classifier_model()\n",
    "        self.run_knn_classifier_model()\n",
    "        self.run_ada_boost_classifier_model()\n",
    "        self.run_xg_boost_classifier_model()\n",
    "        # self.run_ann_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62482853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w500_o50_features.csv\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlKernel",
   "language": "python",
   "name": "mlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
