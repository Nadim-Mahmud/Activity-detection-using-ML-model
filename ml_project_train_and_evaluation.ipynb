{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218aeb79",
   "metadata": {},
   "source": [
    "## Predication with Different Classification Method to The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cecb0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Ensemble Methods\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df5f1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b276802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(obj, p, cycle)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set max output lines before scrolling\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.instance().display_formatter.formatters['text/plain'].for_type(\n",
    "    type, lambda obj, p, cycle: p.text(repr(obj)[:10000])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941cad",
   "metadata": {},
   "source": [
    "### Metrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aceecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric Calculations\n",
    "\n",
    "def calculate_metrics(classifier, y_val, y_pred):\n",
    "    print(f\"{classifier} metrics: \")\n",
    "\n",
    "    print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9677b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_accuracy_gen(model, label, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    calculate_metrics(label, y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a91ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationPipeline:\n",
    "\n",
    "    param_grid_logistic_regression = {\n",
    "        'C': [0.01, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    param_grid_decission_tree_classifier = {\n",
    "        'max_depth': [None, 5, 20, 50],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "\n",
    "    param_grid_random_forest_classifier = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 20, 50],\n",
    "        'bootstrap': [True, False],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    param_grid_gaussian_naive_bias = {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "    }\n",
    "\n",
    "    param_grid_svc = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'kernel': ['rbf', 'poly']\n",
    "    }\n",
    "\n",
    "    param_grid_knn = {\n",
    "        'n_neighbors': [100, 500, 700, 900, 1100, 1500],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    param_grid_ada_boost = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.5, 1.0],\n",
    "        'estimator': [\n",
    "            DecisionTreeClassifier(max_depth=1),\n",
    "            DecisionTreeClassifier(max_depth=3)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2], \n",
    "        'subsample': [0.6, 0.8, 1.0],     \n",
    "        'gamma': [0, 0.1, 0.3, 0.5],           \n",
    "    }\n",
    "\n",
    "    param_grid_ann = {\n",
    "        'model__n_neurons': [32, 64, 128],\n",
    "        'model__activation': ['relu', 'tanh'],\n",
    "        'epochs': [10, 20],\n",
    "        'batch_size': [16, 32, 64]\n",
    "    }\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.feature_path = file_path\n",
    "        self.feature_df = self.get_feture()\n",
    "        self.X, self.y = self.split_feture_and_target()\n",
    "        self.y = self.map_zero_to_n() # mapping y zero to number of class to make it usable for some modles i.e. xgaboost\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.get_scale_and_test_train_split()\n",
    "\n",
    "    # data read and processing section\n",
    "    def remove_outliear(self, feature_df):\n",
    "        iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "        outliers = iso.fit_predict(feature_df)\n",
    "        data_cleaned = feature_df[outliers == 1]\n",
    "\n",
    "        return data_cleaned\n",
    "\n",
    "    def get_feture(self):\n",
    "        feature_df = pd.read_csv(self.feature_path)\n",
    "        feature_df = feature_df.iloc[:, 1:] # remove index\n",
    "        \n",
    "        return self.remove_outliear(feature_df)\n",
    "\n",
    "    def split_feture_and_target(self):\n",
    "        X = self.feature_df.iloc[:, :-1]\n",
    "        y = self.feature_df.iloc[:, -1]\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def get_scale_and_test_train_split(self):\n",
    "        #Scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaled_fature = scaler.fit_transform(self.X)\n",
    "        \n",
    "        #test train split\n",
    "        return train_test_split(scaled_fature, self.y, train_size=.20, random_state=42, stratify=self.y)\n",
    "    \n",
    "    def map_zero_to_n(self):\n",
    "        unique_values = {val: idx for idx, val in enumerate(self.y.unique())}\n",
    "        y_mapped = self.y.map(unique_values)\n",
    "\n",
    "        return y_mapped\n",
    "    \n",
    "    # Cross validation section \n",
    "    def kfold_cross_validation(self, model, n_splits):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        kfold_score = cross_val_score(model, self.X, self.y, cv=kf)\n",
    "\n",
    "        print(\"K-fold cross validaiton scores:\", kfold_score)\n",
    "\n",
    "    def stratified_cross_validation(self, model, n_splits):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        skfold_score = cross_val_score(model, self.X, self.y, cv=skf)\n",
    "\n",
    "        print(\"Straified cross validation scores:\", skfold_score)\n",
    "\n",
    "    def cross_validation(self, model, n_splits):\n",
    "        self.kfold_cross_validation(model, n_splits)\n",
    "        self.stratified_cross_validation(model, n_splits)\n",
    "\n",
    "    # Hyper parameter tuning\n",
    "\n",
    "    def gridSerach(self, estimator, param_grid):\n",
    "        print(\"==== Grid Search: =====\")\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3, verbose=0)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Best parameters found: \", grid_search.best_params_)\n",
    "        print(\"Best score found: \", grid_search.best_score_)\n",
    "\n",
    "        return grid_search\n",
    "    \n",
    "    def randomSearch(self, estimator, param_grid):\n",
    "        print(\"\\n==== Random Search: =====\")\n",
    "\n",
    "        random_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=500, cv=3, random_state=42)\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Best parameters found: \", random_search.best_params_)\n",
    "        print(\"Best score found: \", random_search.best_score_)\n",
    "\n",
    "        return random_search\n",
    "    \n",
    "    def hyper_parameter_tuning(self, model, param_grid):\n",
    "        grid_search = self.gridSerach(model, param_grid)\n",
    "        random_search = self.randomSearch(model, param_grid)\n",
    "\n",
    "        return grid_search if grid_search.best_score_ > random_search.best_score_ else random_search\n",
    "    \n",
    "    # Models section\n",
    "    def run_logistic_regression_model(self):\n",
    "        print(\"=============== 1. Logistic Regression Section: ==================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(LogisticRegression(), self.param_grid_logistic_regression)\n",
    "        lrm = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(lrm, \"1. Logistic regression\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(lrm, 10)\n",
    "    \n",
    "    def run_decission_tree_classifier_model(self):\n",
    "        print(\"=================2. Decission Tree Classifier Section: ================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(DecisionTreeClassifier(), self.param_grid_decission_tree_classifier)\n",
    "        dt = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(dt, \"2. Decission Tree Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(dt, 10)\n",
    "\n",
    "    def run_random_forest_classifier_model(self):\n",
    "        print(\"=================== 3. Random Forest Classifier Section: ==================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(RandomForestClassifier(), self.param_grid_random_forest_classifier)\n",
    "        rfc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(rfc, \"3.  Random Forest Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(rfc, 10)\n",
    "\n",
    "    def run_gaussian_naive_bias_classifier_model(self):\n",
    "        print(\"=================== 4. Gaussian Naive Bias Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(GaussianNB(), self.param_grid_gaussian_naive_bias)\n",
    "        gnb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(gnb, \"4. Gaussian Naive Bias Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(gnb, 10)\n",
    "\n",
    "\n",
    "    def run_support_vector_classifier_model(self):\n",
    "        print(\"=================== 5. Support Vector Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(SVC(), self.param_grid_svc)\n",
    "        svc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(svc, \"5. Support Vector Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(svc, 10)\n",
    "\n",
    "\n",
    "    def run_knn_classifier_model(self):\n",
    "        print(\"=================== 6. K-Nearest Neighbors Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(KNeighborsClassifier(), self.param_grid_knn)\n",
    "        knn = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(knn, \"6. K-Nearest Neighbors\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(knn, 10)\n",
    "\n",
    "    def run_ada_boost_classifier_model(self):\n",
    "        print(\"=================== 7. Ada Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(AdaBoostClassifier(), self.param_grid_ada_boost)\n",
    "        abc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(abc, \"7. Ada Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(abc, 10)\n",
    "\n",
    "    def run_xg_boost_classifier_model(self):\n",
    "        print(\"=================== 8. XG Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(XGBClassifier(), self.param_grid_xgb)\n",
    "        xgb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(xgb, \"8. XG Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(xgb, 10)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ann(n_neurons=64, activation='relu'):\n",
    "        model = Sequential()\n",
    "        # Input layer\n",
    "        model.add(Dense(n_neurons, activation=activation, input_shape=(24,)))\n",
    "        \n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "            \n",
    "        # Output layer (example for binary classification)\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def run_ann_model(self):\n",
    "        print(\"=================== 9. Artificial Neural Net Section: ===================\")\n",
    "\n",
    "        model = KerasClassifier(build_fn=self.build_ann, verbose=0)\n",
    "        \n",
    "        tuned_model = self.hyper_parameter_tuning(model, self.param_grid_ann)\n",
    "        ann = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(ann, \"9. Artificial Neuralnet\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(ann, 10)\n",
    "\n",
    "    def driver(self):\n",
    "        self.run_logistic_regression_model()\n",
    "        self.run_decission_tree_classifier_model()\n",
    "        self.run_random_forest_classifier_model()\n",
    "        self.run_gaussian_naive_bias_classifier_model()\n",
    "        self.run_support_vector_classifier_model()\n",
    "        self.run_knn_classifier_model()\n",
    "        self.run_ada_boost_classifier_model()\n",
    "        self.run_xg_boost_classifier_model()\n",
    "        # self.run_ann_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772b588",
   "metadata": {},
   "source": [
    "# == Model and scores for Window 100 & 25% Overlap ==\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57df36a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== 1. Logistic Regression Section: ==================\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'C': 100, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best score found:  0.30028598976074977\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 100, 'C': 100}\n",
      "Best score found:  0.30028598976074977\n",
      "1. Logistic regression metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.41      0.37      2113\n",
      "           1       0.21      0.21      0.21      2267\n",
      "           2       0.14      0.16      0.15      2174\n",
      "           3       0.23      0.35      0.27      2224\n",
      "           4       0.91      0.74      0.81      1721\n",
      "           5       0.16      0.13      0.14      2222\n",
      "           6       0.12      0.01      0.02      2133\n",
      "           7       0.36      0.03      0.05      1649\n",
      "           8       0.60      0.71      0.65      1982\n",
      "           9       0.20      0.25      0.22      2113\n",
      "          10       0.15      0.06      0.09      1788\n",
      "          11       0.23      0.07      0.11      2081\n",
      "          12       0.22      0.29      0.25      2278\n",
      "          13       0.18      0.38      0.24      2161\n",
      "          14       0.59      0.80      0.68      1800\n",
      "\n",
      "    accuracy                           0.30     30706\n",
      "   macro avg       0.31      0.31      0.29     30706\n",
      "weighted avg       0.30      0.30      0.28     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.2453764  0.22193279 0.263679   0.27670662 0.2582074  0.24361647\n",
      " 0.26680563 0.23658155 0.26498176 0.24570089]\n",
      "Straified cross validation scores: [0.25188851 0.24329252 0.25429911 0.26654508 0.26993226 0.25951016\n",
      " 0.25039083 0.2522147  0.24335591 0.26341845]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline = ModelEvaluationPipeline(\"features/w100_o25_features.csv\")\n",
    "w100_o25_pipeline.run_logistic_regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86aeece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================2. Decission Tree Classifier Section: ================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5}\n",
      "Best score found:  0.5052100121775562\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'min_samples_split': 10, 'max_depth': None, 'criterion': 'entropy'}\n",
      "Best score found:  0.504296924609449\n",
      "2. Decission Tree Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.34      0.32      2113\n",
      "           1       0.46      0.47      0.46      2267\n",
      "           2       0.41      0.43      0.42      2174\n",
      "           3       0.51      0.59      0.54      2224\n",
      "           4       0.79      0.82      0.80      1721\n",
      "           5       0.37      0.40      0.39      2222\n",
      "           6       0.39      0.38      0.38      2133\n",
      "           7       0.34      0.33      0.34      1649\n",
      "           8       0.65      0.66      0.66      1982\n",
      "           9       0.81      0.79      0.80      2113\n",
      "          10       0.40      0.40      0.40      1788\n",
      "          11       0.26      0.24      0.25      2081\n",
      "          12       0.61      0.52      0.56      2278\n",
      "          13       0.72      0.66      0.69      2161\n",
      "          14       0.70      0.66      0.68      1800\n",
      "\n",
      "    accuracy                           0.51     30706\n",
      "   macro avg       0.52      0.51      0.51     30706\n",
      "weighted avg       0.51      0.51      0.51     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.60901276 0.59911435 0.60135487 0.59431996 0.60630537 0.59718603\n",
      " 0.60604482 0.59927045 0.59927045 0.60317874]\n",
      "Straified cross validation scores: [0.60302162 0.60302162 0.60786868 0.59927045 0.60083377 0.59536217\n",
      " 0.59588327 0.61933299 0.61021365 0.59588327]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_decission_tree_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8436278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 3. Random Forest Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 500}\n",
      "Best score found:  0.6263648828486906\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'n_estimators': 500, 'max_depth': 20, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Best score found:  0.625974105201172\n",
      "3.  Random Forest Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.60      0.49      2113\n",
      "           1       0.61      0.66      0.63      2267\n",
      "           2       0.68      0.47      0.56      2174\n",
      "           3       0.65      0.74      0.69      2224\n",
      "           4       0.91      0.85      0.88      1721\n",
      "           5       0.50      0.56      0.53      2222\n",
      "           6       0.66      0.42      0.51      2133\n",
      "           7       0.56      0.43      0.48      1649\n",
      "           8       0.67      0.83      0.74      1982\n",
      "           9       0.85      0.90      0.87      2113\n",
      "          10       0.57      0.52      0.54      1788\n",
      "          11       0.44      0.30      0.36      2081\n",
      "          12       0.70      0.70      0.70      2278\n",
      "          13       0.76      0.81      0.78      2161\n",
      "          14       0.71      0.84      0.77      1800\n",
      "\n",
      "    accuracy                           0.64     30706\n",
      "   macro avg       0.65      0.64      0.64     30706\n",
      "weighted avg       0.64      0.64      0.64     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.72701224 0.72154207 0.72668056 0.71521626 0.72146952 0.71599792\n",
      " 0.73163106 0.7199062  0.73397603 0.7193851 ]\n",
      "Straified cross validation scores: [0.71971868 0.72050013 0.72120896 0.72329338 0.72303283 0.73137051\n",
      " 0.72954664 0.71860344 0.72589891 0.72068786]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_random_forest_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "392cc94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 4. Gaussian Naive Bias Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'var_smoothing': 0.0001}\n",
      "Best score found:  0.22420539077611987\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'var_smoothing': 0.0001}\n",
      "Best score found:  0.22420539077611987\n",
      "4. Gaussian Naive Bias Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.13      0.18      2113\n",
      "           1       0.23      0.14      0.17      2267\n",
      "           2       0.10      0.01      0.01      2174\n",
      "           3       0.17      0.01      0.03      2224\n",
      "           4       0.83      0.72      0.77      1721\n",
      "           5       0.13      0.08      0.10      2222\n",
      "           6       0.13      0.01      0.03      2133\n",
      "           7       0.12      0.03      0.04      1649\n",
      "           8       0.47      0.58      0.52      1982\n",
      "           9       0.13      0.03      0.04      2113\n",
      "          10       0.12      0.05      0.07      1788\n",
      "          11       0.17      0.07      0.10      2081\n",
      "          12       0.13      0.09      0.11      2278\n",
      "          13       0.12      0.89      0.21      2161\n",
      "          14       0.54      0.63      0.58      1800\n",
      "\n",
      "    accuracy                           0.22     30706\n",
      "   macro avg       0.24      0.23      0.20     30706\n",
      "weighted avg       0.23      0.22      0.19     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.13076322 0.12763741 0.13288171 0.13079729 0.1378322  0.12949453\n",
      " 0.12272017 0.13574779 0.13288171 0.12662845]\n",
      "Straified cross validation scores: [0.13180516 0.12998177 0.13157895 0.126889   0.13288171 0.12897342\n",
      " 0.13548723 0.12819177 0.12819177 0.13340281]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_gaussian_naive_bias_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99a0c6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 5. Support Vector Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best score found:  0.5385602924487439\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'kernel': 'rbf', 'gamma': 0.1, 'C': 1000}\n",
      "Best score found:  0.5385602924487439\n",
      "5. Support Vector Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.47      0.40      2113\n",
      "           1       0.56      0.57      0.56      2267\n",
      "           2       0.40      0.45      0.43      2174\n",
      "           3       0.56      0.68      0.61      2224\n",
      "           4       0.84      0.83      0.84      1721\n",
      "           5       0.45      0.43      0.44      2222\n",
      "           6       0.37      0.28      0.32      2133\n",
      "           7       0.43      0.34      0.38      1649\n",
      "           8       0.57      0.79      0.66      1982\n",
      "           9       0.80      0.83      0.81      2113\n",
      "          10       0.53      0.41      0.46      1788\n",
      "          11       0.35      0.24      0.29      2081\n",
      "          12       0.62      0.58      0.60      2278\n",
      "          13       0.77      0.71      0.74      2161\n",
      "          14       0.73      0.75      0.74      1800\n",
      "\n",
      "    accuracy                           0.56     30706\n",
      "   macro avg       0.56      0.56      0.55     30706\n",
      "weighted avg       0.55      0.56      0.55     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.26855952 0.27142485 0.28452319 0.27696717 0.27774883 0.27540386\n",
      " 0.27853048 0.28217822 0.27618551 0.26550287]\n",
      "Straified cross validation scores: [0.27142485 0.27142485 0.28295987 0.28191767 0.27696717 0.27774883\n",
      " 0.27488275 0.26628452 0.27331944 0.26862949]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_support_vector_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8578ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 6. K-Nearest Neighbors Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'metric': 'minkowski', 'n_neighbors': 100, 'p': 1, 'weights': 'distance'}\n",
      "Best score found:  0.5031243370554471\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'weights': 'distance', 'p': 1, 'n_neighbors': 100, 'metric': 'minkowski'}\n",
      "Best score found:  0.5031243370554471\n",
      "6. K-Nearest Neighbors metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.28      0.32      2113\n",
      "           1       0.58      0.49      0.53      2267\n",
      "           2       0.60      0.43      0.50      2174\n",
      "           3       0.47      0.74      0.57      2224\n",
      "           4       0.91      0.74      0.81      1721\n",
      "           5       0.34      0.45      0.39      2222\n",
      "           6       0.72      0.34      0.46      2133\n",
      "           7       0.48      0.25      0.33      1649\n",
      "           8       0.55      0.65      0.59      1982\n",
      "           9       0.66      0.84      0.74      2113\n",
      "          10       0.52      0.30      0.38      1788\n",
      "          11       0.38      0.16      0.23      2081\n",
      "          12       0.45      0.72      0.55      2278\n",
      "          13       0.60      0.61      0.61      2161\n",
      "          14       0.47      0.85      0.61      1800\n",
      "\n",
      "    accuracy                           0.52     30706\n",
      "   macro avg       0.54      0.52      0.51     30706\n",
      "weighted avg       0.54      0.52      0.51     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.36207346 0.35712425 0.373111   0.36425221 0.36868161 0.35617509\n",
      " 0.36842105 0.35982282 0.37623762 0.34445023]\n",
      "Straified cross validation scores: [0.3714509  0.35399844 0.36008338 0.37571652 0.36529442 0.36190724\n",
      " 0.3681605  0.36008338 0.36034393 0.3741532 ]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_knn_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5360ffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 7. Ada Boost Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 0.01, 'n_estimators': 100}\n",
      "Best score found:  0.3444527244086726\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'n_estimators': 100, 'learning_rate': 0.01, 'estimator': DecisionTreeClassifier(max_depth=3)}\n",
      "Best score found:  0.3444527244086726\n",
      "7. Ada Boost Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.62      0.37      2113\n",
      "           1       0.30      0.44      0.35      2267\n",
      "           2       0.57      0.03      0.06      2174\n",
      "           3       0.23      0.29      0.26      2224\n",
      "           4       0.00      0.00      0.00      1721\n",
      "           5       0.18      0.20      0.19      2222\n",
      "           6       0.23      0.00      0.00      2133\n",
      "           7       0.35      0.10      0.15      1649\n",
      "           8       0.38      0.76      0.51      1982\n",
      "           9       0.62      0.37      0.46      2113\n",
      "          10       0.23      0.04      0.06      1788\n",
      "          11       0.15      0.02      0.03      2081\n",
      "          12       0.22      0.63      0.33      2278\n",
      "          13       0.39      0.42      0.40      2161\n",
      "          14       0.64      0.75      0.69      1800\n",
      "\n",
      "    accuracy                           0.32     30706\n",
      "   macro avg       0.32      0.31      0.26     30706\n",
      "weighted avg       0.32      0.32      0.26     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.31388382 0.32352175 0.33090151 0.31605003 0.32595102 0.3238666\n",
      " 0.32647212 0.32490881 0.33324648 0.3238666 ]\n",
      "Straified cross validation scores: [0.32560563 0.32378223 0.3293382  0.31969776 0.32647212 0.32021886\n",
      " 0.3243877  0.31474726 0.33090151 0.31813445]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_ada_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89859406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 8. XG Boost Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'gamma': 0, 'learning_rate': 0.2, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best score found:  0.6232385598239637\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'subsample': 0.8, 'n_estimators': 200, 'learning_rate': 0.2, 'gamma': 0}\n",
      "Best score found:  0.6232385598239637\n",
      "8. XG Boost Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.55      0.50      2113\n",
      "           1       0.63      0.64      0.63      2267\n",
      "           2       0.65      0.52      0.58      2174\n",
      "           3       0.65      0.72      0.68      2224\n",
      "           4       0.91      0.85      0.88      1721\n",
      "           5       0.47      0.53      0.50      2222\n",
      "           6       0.60      0.46      0.52      2133\n",
      "           7       0.51      0.41      0.45      1649\n",
      "           8       0.74      0.82      0.78      1982\n",
      "           9       0.85      0.89      0.87      2113\n",
      "          10       0.55      0.52      0.54      1788\n",
      "          11       0.38      0.35      0.37      2081\n",
      "          12       0.69      0.68      0.68      2278\n",
      "          13       0.76      0.81      0.78      2161\n",
      "          14       0.76      0.85      0.80      1800\n",
      "\n",
      "    accuracy                           0.64     30706\n",
      "   macro avg       0.64      0.64      0.64     30706\n",
      "weighted avg       0.64      0.64      0.64     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.72987757 0.72180255 0.70948411 0.71365294 0.72199062 0.71313184\n",
      " 0.72720167 0.71417405 0.72511725 0.71365294]\n",
      "Straified cross validation scores: [0.70773639 0.71711383 0.70922355 0.71313184 0.71287129 0.72746222\n",
      " 0.71756123 0.71912454 0.72277228 0.71261073]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_xg_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dea4b3",
   "metadata": {},
   "source": [
    "# == Model and scores for Window 100 & 50% Overlap =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76401191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== 1. Logistic Regression Section: ==================\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'C': 100, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best score found:  0.2812825860271116\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 100, 'C': 100}\n",
      "Best score found:  0.2812825860271116\n",
      "1. Logistic regression metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.40      0.36      3172\n",
      "           1       0.22      0.22      0.22      3402\n",
      "           2       0.11      0.09      0.10      3254\n",
      "           3       0.20      0.38      0.26      3333\n",
      "           4       0.93      0.75      0.83      2557\n",
      "           5       0.16      0.12      0.14      3332\n",
      "           6       0.17      0.01      0.02      3208\n",
      "           7       0.32      0.01      0.02      2466\n",
      "           8       0.62      0.72      0.66      2984\n",
      "           9       0.16      0.13      0.15      3168\n",
      "          10       0.15      0.05      0.07      2681\n",
      "          11       0.27      0.10      0.14      3122\n",
      "          12       0.19      0.27      0.22      3415\n",
      "          13       0.16      0.39      0.23      3241\n",
      "          14       0.59      0.80      0.68      2697\n",
      "\n",
      "    accuracy                           0.29     46032\n",
      "   macro avg       0.30      0.30      0.27     46032\n",
      "weighted avg       0.29      0.29      0.26     46032\n",
      "\n",
      "K-fold cross validaiton scores: [0.2455683  0.25147723 0.23792145 0.23305527 0.25530066 0.25147723\n",
      " 0.25182482 0.23409802 0.26659715 0.256691  ]\n",
      "Straified cross validation scores: [0.25981926 0.24921794 0.21724018 0.24730622 0.24209246 0.24139729\n",
      " 0.23583594 0.25477928 0.20611748 0.25321515]\n"
     ]
    }
   ],
   "source": [
    "w100_o50_pipeline = ModelEvaluationPipeline(\"features/w100_o50_features.csv\")\n",
    "w100_o50_pipeline.run_logistic_regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32154551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================2. Decission Tree Classifier Section: ================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 20}\n",
      "Best score found:  0.5251129648939868\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'min_samples_split': 20, 'max_depth': 20, 'criterion': 'gini'}\n",
      "Best score found:  0.5264164059784499\n",
      "2. Decission Tree Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.42      0.40      3172\n",
      "           1       0.46      0.55      0.50      3402\n",
      "           2       0.47      0.42      0.45      3254\n",
      "           3       0.60      0.62      0.61      3333\n",
      "           4       0.80      0.80      0.80      2557\n",
      "           5       0.38      0.48      0.42      3332\n",
      "           6       0.48      0.41      0.44      3208\n",
      "           7       0.39      0.35      0.37      2466\n",
      "           8       0.61      0.65      0.63      2984\n",
      "           9       0.78      0.79      0.79      3168\n",
      "          10       0.44      0.42      0.43      2681\n",
      "          11       0.28      0.25      0.27      3122\n",
      "          12       0.58      0.56      0.57      3415\n",
      "          13       0.72      0.65      0.68      3241\n",
      "          14       0.74      0.67      0.70      2697\n",
      "\n",
      "    accuracy                           0.53     46032\n",
      "   macro avg       0.54      0.54      0.54     46032\n",
      "weighted avg       0.54      0.53      0.54     46032\n",
      "\n",
      "K-fold cross validaiton scores: [0.62634689 0.63017032 0.62443518 0.61696211 0.63347237 0.6261731\n",
      " 0.62530414 0.62043796 0.62547793 0.62026416]\n",
      "Straified cross validation scores: [0.63955509 0.61609315 0.62547793 0.63607925 0.62843239 0.63017032\n",
      " 0.6173097  0.62565172 0.6150504  0.62356621]\n"
     ]
    }
   ],
   "source": [
    "w100_o50_pipeline.run_decission_tree_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "138ae675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 3. Random Forest Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 500}\n",
      "Best score found:  0.6624087591240876\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'n_estimators': 500, 'max_depth': 50, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Best score found:  0.6632777198470629\n",
      "3.  Random Forest Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.62      0.51      3172\n",
      "           1       0.63      0.73      0.67      3402\n",
      "           2       0.76      0.54      0.63      3254\n",
      "           3       0.73      0.76      0.75      3333\n",
      "           4       0.92      0.88      0.90      2557\n",
      "           5       0.56      0.61      0.59      3332\n",
      "           6       0.73      0.47      0.57      3208\n",
      "           7       0.60      0.49      0.54      2466\n",
      "           8       0.70      0.84      0.76      2984\n",
      "           9       0.87      0.91      0.89      3168\n",
      "          10       0.65      0.58      0.61      2681\n",
      "          11       0.53      0.37      0.44      3122\n",
      "          12       0.71      0.78      0.74      3415\n",
      "          13       0.77      0.82      0.79      3241\n",
      "          14       0.75      0.84      0.79      2697\n",
      "\n",
      "    accuracy                           0.68     46032\n",
      "   macro avg       0.69      0.68      0.68     46032\n",
      "weighted avg       0.69      0.68      0.68     46032\n",
      "\n",
      "K-fold cross validaiton scores: [0.77024678 0.76572819 0.76451164 0.75773375 0.77163712 0.76850886\n",
      " 0.76520681 0.76694473 0.76711853 0.76138339]\n",
      "Straified cross validation scores: [0.77754605 0.76451164 0.76711853 0.76729232 0.756691   0.76485923\n",
      " 0.76468544 0.76624957 0.77042058 0.76885645]\n"
     ]
    }
   ],
   "source": [
    "w100_o50_pipeline.run_random_forest_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e01462d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 4. Gaussian Naive Bias Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'var_smoothing': 0.0001}\n",
      "Best score found:  0.24991310392770247\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'var_smoothing': 0.0001}\n",
      "Best score found:  0.24991310392770247\n",
      "4. Gaussian Naive Bias Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.16      0.21      3172\n",
      "           1       0.23      0.13      0.16      3402\n",
      "           2       0.22      0.02      0.03      3254\n",
      "           3       0.19      0.55      0.28      3333\n",
      "           4       0.85      0.73      0.78      2557\n",
      "           5       0.13      0.09      0.10      3332\n",
      "           6       0.21      0.02      0.03      3208\n",
      "           7       0.09      0.03      0.04      2466\n",
      "           8       0.49      0.59      0.53      2984\n",
      "           9       0.18      0.02      0.04      3168\n",
      "          10       0.11      0.06      0.07      2681\n",
      "          11       0.19      0.05      0.08      3122\n",
      "          12       0.11      0.10      0.11      3415\n",
      "          13       0.18      0.80      0.29      3241\n",
      "          14       0.60      0.65      0.62      2697\n",
      "\n",
      "    accuracy                           0.26     46032\n",
      "   macro avg       0.27      0.27      0.23     46032\n",
      "weighted avg       0.26      0.26      0.22     46032\n",
      "\n",
      "K-fold cross validaiton scores: [0.12895377 0.13468891 0.1216545  0.13103928 0.12182829 0.12652068\n",
      " 0.13069169 0.1282586  0.13312478 0.12339242]\n",
      "Straified cross validation scores: [0.12721585 0.12356621 0.1282586  0.12252346 0.12964894 0.1348627\n",
      " 0.12912756 0.12252346 0.12547793 0.13416754]\n"
     ]
    }
   ],
   "source": [
    "w100_o50_pipeline.run_gaussian_naive_bias_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b3ed267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 5. Support Vector Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best score found:  0.5652589502954467\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'kernel': 'rbf', 'gamma': 0.1, 'C': 1000}\n",
      "Best score found:  0.5652589502954467\n",
      "5. Support Vector Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.48      0.42      3172\n",
      "           1       0.59      0.60      0.60      3402\n",
      "           2       0.41      0.39      0.40      3254\n",
      "           3       0.59      0.70      0.64      3333\n",
      "           4       0.84      0.83      0.84      2557\n",
      "           5       0.48      0.49      0.49      3332\n",
      "           6       0.39      0.32      0.35      3208\n",
      "           7       0.47      0.40      0.44      2466\n",
      "           8       0.58      0.80      0.67      2984\n",
      "           9       0.79      0.87      0.83      3168\n",
      "          10       0.60      0.44      0.51      2681\n",
      "          11       0.44      0.28      0.34      3122\n",
      "          12       0.60      0.66      0.63      3415\n",
      "          13       0.80      0.70      0.75      3241\n",
      "          14       0.78      0.75      0.76      2697\n",
      "\n",
      "    accuracy                           0.58     46032\n",
      "   macro avg       0.58      0.58      0.58     46032\n",
      "weighted avg       0.58      0.58      0.57     46032\n",
      "\n",
      "K-fold cross validaiton scores: [0.28693083 0.29040667 0.29023288 0.28084811 0.29509906 0.30257212\n",
      " 0.30066041 0.29631561 0.29666319 0.30257212]\n",
      "Straified cross validation scores: [0.30344108 0.29075426 0.29127563 0.28849496 0.28571429 0.29509906\n",
      " 0.29127563 0.30709072 0.28745221 0.29075426]\n"
     ]
    }
   ],
   "source": [
    "w100_o50_pipeline.run_support_vector_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eba9d14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 6. K-Nearest Neighbors Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'metric': 'minkowski', 'n_neighbors': 100, 'p': 1, 'weights': 'distance'}\n",
      "Best score found:  0.5222453945081682\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'weights': 'distance', 'p': 1, 'n_neighbors': 100, 'metric': 'minkowski'}\n",
      "Best score found:  0.5222453945081682\n",
      "6. K-Nearest Neighbors metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.30      0.34      3172\n",
      "           1       0.53      0.52      0.53      3402\n",
      "           2       0.64      0.46      0.53      3254\n",
      "           3       0.49      0.73      0.58      3333\n",
      "           4       0.92      0.77      0.84      2557\n",
      "           5       0.37      0.47      0.41      3332\n",
      "           6       0.73      0.35      0.47      3208\n",
      "           7       0.52      0.27      0.36      2466\n",
      "           8       0.58      0.68      0.63      2984\n",
      "           9       0.68      0.88      0.77      3168\n",
      "          10       0.61      0.37      0.46      2681\n",
      "          11       0.40      0.17      0.24      3122\n",
      "          12       0.48      0.75      0.58      3415\n",
      "          13       0.64      0.64      0.64      3241\n",
      "          14       0.50      0.86      0.64      2697\n",
      "\n",
      "    accuracy                           0.55     46032\n",
      "   macro avg       0.57      0.55      0.53     46032\n",
      "weighted avg       0.56      0.55      0.53     46032\n",
      "\n",
      "K-fold cross validaiton scores: [0.37938825 0.37643379 0.37643379 0.36809176 0.38877303 0.3892944\n",
      " 0.37573862 0.38390685 0.37226277 0.36687522]\n",
      "Straified cross validation scores: [0.38112617 0.37678137 0.37069864 0.37782412 0.37087244 0.37799791\n",
      " 0.37330553 0.39363921 0.37208898 0.38338547]\n"
     ]
    }
   ],
   "source": [
    "w100_o50_pipeline.run_knn_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_ada_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_xg_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685b012",
   "metadata": {},
   "source": [
    "# == Model and scores for Window 200 & 25% Overlap =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "w200_o25_pipeline = ModelEvaluationPipeline(\"features/w200_o25_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf29562",
   "metadata": {},
   "outputs": [],
   "source": [
    "w200_o25_pipeline.run_logistic_regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1fa735",
   "metadata": {},
   "outputs": [],
   "source": [
    "w200_o25_pipeline.run_random_forest_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w200_o25_pipeline.run_decission_tree_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "w200_o25_pipeline.run_gaussian_naive_bias_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4058950",
   "metadata": {},
   "outputs": [],
   "source": [
    "w200_o25_pipeline.run_knn_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w200_o25_pipeline.run_ada_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w200_o25_pipeline.run_xg_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0693fa",
   "metadata": {},
   "source": [
    "# == Model and scores for Window 200 & 50% Overlap =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w200_o50_features.csv\", \"Window 200 & 50% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w300_o25_features.csv\", \"Window 300 & 25% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdede7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w300_o50_features.csv\", \"Window 300 & 50% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w400_o25_features.csv\", \"Window 400 & 25% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w400_o50_features.csv\", \"Window 400 & 50% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w500_o25_features.csv\", \"Window 500 & 25% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62482853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w500_o50_features.csv\", \"Window 500 & 50% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlKernel",
   "language": "python",
   "name": "mlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
