{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218aeb79",
   "metadata": {},
   "source": [
    "## Predication with Different Classification Method to The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cecb0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Ensemble Methods\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df5f1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b276802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(obj, p, cycle)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set max output lines before scrolling\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.instance().display_formatter.formatters['text/plain'].for_type(\n",
    "    type, lambda obj, p, cycle: p.text(repr(obj)[:10000])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941cad",
   "metadata": {},
   "source": [
    "### Metrics Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aceecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric Calculations\n",
    "\n",
    "def calculate_metrics(classifier, y_val, y_pred):\n",
    "    print(f\"{classifier} metrics: \")\n",
    "\n",
    "    print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9677b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_accuracy_gen(model, label, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    calculate_metrics(label, y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a91ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluationPipeline:\n",
    "\n",
    "    param_grid_logistic_regression = {\n",
    "        'C': [0.01, 1, 10, 100],\n",
    "        'solver': ['lbfgs', 'liblinear', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    param_grid_decission_tree_classifier = {\n",
    "        'max_depth': [None, 5, 20, 50],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "    }\n",
    "\n",
    "    param_grid_random_forest_classifier = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 10, 20, 50],\n",
    "        'bootstrap': [True, False],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    param_grid_gaussian_naive_bias = {\n",
    "        'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]\n",
    "    }\n",
    "\n",
    "    param_grid_svc = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "        'kernel': ['rbf', 'poly']\n",
    "    }\n",
    "\n",
    "    param_grid_knn = {\n",
    "        'n_neighbors': [100, 500, 700, 900, 1100, 1500],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "\n",
    "    param_grid_ada_boost = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.5, 1.0],\n",
    "        'estimator': [\n",
    "            DecisionTreeClassifier(max_depth=1),\n",
    "            DecisionTreeClassifier(max_depth=3)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2], \n",
    "        'subsample': [0.6, 0.8, 1.0],     \n",
    "        'gamma': [0, 0.1, 0.3, 0.5],           \n",
    "    }\n",
    "\n",
    "    param_grid_ann = {\n",
    "        'model__n_neurons': [32, 64, 128],\n",
    "        'model__activation': ['relu', 'tanh'],\n",
    "        'epochs': [10, 20],\n",
    "        'batch_size': [16, 32, 64]\n",
    "    }\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.feature_path = file_path\n",
    "        self.feature_df = self.get_feture()\n",
    "        self.X, self.y = self.split_feture_and_target()\n",
    "        self.y = self.map_zero_to_n() # mapping y zero to number of class to make it usable for some modles i.e. xgaboost\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.get_scale_and_test_train_split()\n",
    "\n",
    "    # data read and processing section\n",
    "    def remove_outliear(self, feature_df):\n",
    "        iso = IsolationForest(contamination=0.01, random_state=42)\n",
    "        outliers = iso.fit_predict(feature_df)\n",
    "        data_cleaned = feature_df[outliers == 1]\n",
    "\n",
    "        return data_cleaned\n",
    "\n",
    "    def get_feture(self):\n",
    "        feature_df = pd.read_csv(self.feature_path)\n",
    "        feature_df = feature_df.iloc[:, 1:] # remove index\n",
    "        \n",
    "        return self.remove_outliear(feature_df)\n",
    "\n",
    "    def split_feture_and_target(self):\n",
    "        X = self.feature_df.iloc[:, :-1]\n",
    "        y = self.feature_df.iloc[:, -1]\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def get_scale_and_test_train_split(self):\n",
    "        #Scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaled_fature = scaler.fit_transform(self.X)\n",
    "        \n",
    "        #test train split\n",
    "        return train_test_split(scaled_fature, self.y, train_size=.20, random_state=42, stratify=self.y)\n",
    "    \n",
    "    def map_zero_to_n(self):\n",
    "        unique_values = {val: idx for idx, val in enumerate(self.y.unique())}\n",
    "        y_mapped = self.y.map(unique_values)\n",
    "\n",
    "        return y_mapped\n",
    "    \n",
    "    # Cross validation section \n",
    "    def kfold_cross_validation(self, model, n_splits):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        kfold_score = cross_val_score(model, self.X, self.y, cv=kf)\n",
    "\n",
    "        print(\"K-fold cross validaiton scores:\", kfold_score)\n",
    "\n",
    "    def stratified_cross_validation(self, model, n_splits):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        skfold_score = cross_val_score(model, self.X, self.y, cv=skf)\n",
    "\n",
    "        print(\"Straified cross validation scores:\", skfold_score)\n",
    "\n",
    "    def cross_validation(self, model, n_splits):\n",
    "        self.kfold_cross_validation(model, n_splits)\n",
    "        self.stratified_cross_validation(model, n_splits)\n",
    "\n",
    "    # Hyper parameter tuning\n",
    "\n",
    "    def gridSerach(self, estimator, param_grid):\n",
    "        print(\"==== Grid Search: =====\")\n",
    "\n",
    "        grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3, verbose=0)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Best parameters found: \", grid_search.best_params_)\n",
    "        print(\"Best score found: \", grid_search.best_score_)\n",
    "\n",
    "        return grid_search\n",
    "    \n",
    "    def randomSearch(self, estimator, param_grid):\n",
    "        print(\"\\n==== Random Search: =====\")\n",
    "\n",
    "        random_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=500, cv=3, random_state=42)\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Best parameters found: \", random_search.best_params_)\n",
    "        print(\"Best score found: \", random_search.best_score_)\n",
    "\n",
    "        return random_search\n",
    "    \n",
    "    def hyper_parameter_tuning(self, model, param_grid):\n",
    "        grid_search = self.gridSerach(model, param_grid)\n",
    "        random_search = self.randomSearch(model, param_grid)\n",
    "\n",
    "        return grid_search if grid_search.best_score_ > random_search.best_score_ else random_search\n",
    "    \n",
    "    # Models section\n",
    "    def run_logistic_regression_model(self):\n",
    "        print(\"=============== 1. Logistic Regression Section: ==================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(LogisticRegression(), self.param_grid_logistic_regression)\n",
    "        lrm = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(lrm, \"1. Logistic regression\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(lrm, 10)\n",
    "    \n",
    "    def run_decission_tree_classifier_model(self):\n",
    "        print(\"=================2. Decission Tree Classifier Section: ================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(DecisionTreeClassifier(), self.param_grid_decission_tree_classifier)\n",
    "        dt = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(dt, \"2. Decission Tree Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(dt, 10)\n",
    "\n",
    "    def run_random_forest_classifier_model(self):\n",
    "        print(\"=================== 3. Random Forest Classifier Section: ==================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(RandomForestClassifier(), self.param_grid_random_forest_classifier)\n",
    "        rfc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(rfc, \"3.  Random Forest Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(rfc, 10)\n",
    "\n",
    "    def run_gaussian_naive_bias_classifier_model(self):\n",
    "        print(\"=================== 4. Gaussian Naive Bias Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(GaussianNB(), self.param_grid_gaussian_naive_bias)\n",
    "        gnb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(gnb, \"4. Gaussian Naive Bias Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(gnb, 10)\n",
    "\n",
    "\n",
    "    def run_support_vector_classifier_model(self):\n",
    "        print(\"=================== 5. Support Vector Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(SVC(), self.param_grid_svc)\n",
    "        svc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(svc, \"5. Support Vector Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(svc, 10)\n",
    "\n",
    "\n",
    "    def run_knn_classifier_model(self):\n",
    "        print(\"=================== 6. K-Nearest Neighbors Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(KNeighborsClassifier(), self.param_grid_knn)\n",
    "        knn = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(knn, \"6. K-Nearest Neighbors\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(knn, 10)\n",
    "\n",
    "    def run_ada_boost_classifier_model(self):\n",
    "        print(\"=================== 7. Ada Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(AdaBoostClassifier(), self.param_grid_ada_boost)\n",
    "        abc = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(abc, \"7. Ada Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(abc, 10)\n",
    "\n",
    "    def run_xg_boost_classifier_model(self):\n",
    "        print(\"=================== 8. XG Boost Classifier Section: ===================\")\n",
    "\n",
    "        tuned_model = self.hyper_parameter_tuning(XGBClassifier(), self.param_grid_xgb)\n",
    "        xgb = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(xgb, \"8. XG Boost Classifier\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(xgb, 10)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ann(n_neurons=64, activation='relu'):\n",
    "        model = Sequential()\n",
    "        # Input layer\n",
    "        model.add(Dense(n_neurons, activation=activation, input_shape=(24,)))\n",
    "        \n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "        model.add(Dense(n_neurons, activation=activation))\n",
    "            \n",
    "        # Output layer (example for binary classification)\n",
    "        model.add(Dense(units=1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def run_ann_model(self):\n",
    "        print(\"=================== 9. Artificial Neural Net Section: ===================\")\n",
    "\n",
    "        model = KerasClassifier(build_fn=self.build_ann, verbose=0)\n",
    "        \n",
    "        tuned_model = self.hyper_parameter_tuning(model, self.param_grid_ann)\n",
    "        ann = tuned_model.best_estimator_\n",
    "\n",
    "        train_and_accuracy_gen(ann, \"9. Artificial Neuralnet\", self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        self.cross_validation(ann, 10)\n",
    "\n",
    "    def driver(self):\n",
    "        self.run_logistic_regression_model()\n",
    "        self.run_decission_tree_classifier_model()\n",
    "        self.run_random_forest_classifier_model()\n",
    "        self.run_gaussian_naive_bias_classifier_model()\n",
    "        self.run_support_vector_classifier_model()\n",
    "        self.run_knn_classifier_model()\n",
    "        self.run_ada_boost_classifier_model()\n",
    "        self.run_xg_boost_classifier_model()\n",
    "        # self.run_ann_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772b588",
   "metadata": {},
   "source": [
    "# == Model and scores for Window 100 & 25% Overlap ==\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57df36a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== 1. Logistic Regression Section: ==================\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'C': 100, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best score found:  0.30028598976074977\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 100, 'C': 100}\n",
      "Best score found:  0.30028598976074977\n",
      "1. Logistic regression metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.41      0.37      2113\n",
      "           1       0.21      0.21      0.21      2267\n",
      "           2       0.14      0.16      0.15      2174\n",
      "           3       0.23      0.35      0.27      2224\n",
      "           4       0.91      0.74      0.81      1721\n",
      "           5       0.16      0.13      0.14      2222\n",
      "           6       0.12      0.01      0.02      2133\n",
      "           7       0.36      0.03      0.05      1649\n",
      "           8       0.60      0.71      0.65      1982\n",
      "           9       0.20      0.25      0.22      2113\n",
      "          10       0.15      0.06      0.09      1788\n",
      "          11       0.23      0.07      0.11      2081\n",
      "          12       0.22      0.29      0.25      2278\n",
      "          13       0.18      0.38      0.24      2161\n",
      "          14       0.59      0.80      0.68      1800\n",
      "\n",
      "    accuracy                           0.30     30706\n",
      "   macro avg       0.31      0.31      0.29     30706\n",
      "weighted avg       0.30      0.30      0.28     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.2453764  0.22193279 0.263679   0.27670662 0.2582074  0.24361647\n",
      " 0.26680563 0.23658155 0.26498176 0.24570089]\n",
      "Straified cross validation scores: [0.25188851 0.24329252 0.25429911 0.26654508 0.26993226 0.25951016\n",
      " 0.25039083 0.2522147  0.24335591 0.26341845]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline = ModelEvaluationPipeline(\"features/w100_o25_features.csv\")\n",
    "w100_o25_pipeline.run_logistic_regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86aeece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================2. Decission Tree Classifier Section: ================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'criterion': 'entropy', 'max_depth': 50, 'min_samples_split': 5}\n",
      "Best score found:  0.5052100121775562\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'min_samples_split': 10, 'max_depth': None, 'criterion': 'entropy'}\n",
      "Best score found:  0.504296924609449\n",
      "2. Decission Tree Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.34      0.32      2113\n",
      "           1       0.46      0.47      0.46      2267\n",
      "           2       0.41      0.43      0.42      2174\n",
      "           3       0.51      0.59      0.54      2224\n",
      "           4       0.79      0.82      0.80      1721\n",
      "           5       0.37      0.40      0.39      2222\n",
      "           6       0.39      0.38      0.38      2133\n",
      "           7       0.34      0.33      0.34      1649\n",
      "           8       0.65      0.66      0.66      1982\n",
      "           9       0.81      0.79      0.80      2113\n",
      "          10       0.40      0.40      0.40      1788\n",
      "          11       0.26      0.24      0.25      2081\n",
      "          12       0.61      0.52      0.56      2278\n",
      "          13       0.72      0.66      0.69      2161\n",
      "          14       0.70      0.66      0.68      1800\n",
      "\n",
      "    accuracy                           0.51     30706\n",
      "   macro avg       0.52      0.51      0.51     30706\n",
      "weighted avg       0.51      0.51      0.51     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.60901276 0.59911435 0.60135487 0.59431996 0.60630537 0.59718603\n",
      " 0.60604482 0.59927045 0.59927045 0.60317874]\n",
      "Straified cross validation scores: [0.60302162 0.60302162 0.60786868 0.59927045 0.60083377 0.59536217\n",
      " 0.59588327 0.61933299 0.61021365 0.59588327]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_decission_tree_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8436278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 3. Random Forest Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 50, 'n_estimators': 500}\n",
      "Best score found:  0.6263648828486906\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'n_estimators': 500, 'max_depth': 20, 'criterion': 'entropy', 'bootstrap': False}\n",
      "Best score found:  0.625974105201172\n",
      "3.  Random Forest Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.60      0.49      2113\n",
      "           1       0.61      0.66      0.63      2267\n",
      "           2       0.68      0.47      0.56      2174\n",
      "           3       0.65      0.74      0.69      2224\n",
      "           4       0.91      0.85      0.88      1721\n",
      "           5       0.50      0.56      0.53      2222\n",
      "           6       0.66      0.42      0.51      2133\n",
      "           7       0.56      0.43      0.48      1649\n",
      "           8       0.67      0.83      0.74      1982\n",
      "           9       0.85      0.90      0.87      2113\n",
      "          10       0.57      0.52      0.54      1788\n",
      "          11       0.44      0.30      0.36      2081\n",
      "          12       0.70      0.70      0.70      2278\n",
      "          13       0.76      0.81      0.78      2161\n",
      "          14       0.71      0.84      0.77      1800\n",
      "\n",
      "    accuracy                           0.64     30706\n",
      "   macro avg       0.65      0.64      0.64     30706\n",
      "weighted avg       0.64      0.64      0.64     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.72701224 0.72154207 0.72668056 0.71521626 0.72146952 0.71599792\n",
      " 0.73163106 0.7199062  0.73397603 0.7193851 ]\n",
      "Straified cross validation scores: [0.71971868 0.72050013 0.72120896 0.72329338 0.72303283 0.73137051\n",
      " 0.72954664 0.71860344 0.72589891 0.72068786]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_random_forest_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "392cc94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 4. Gaussian Naive Bias Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'var_smoothing': 0.0001}\n",
      "Best score found:  0.22420539077611987\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'var_smoothing': 0.0001}\n",
      "Best score found:  0.22420539077611987\n",
      "4. Gaussian Naive Bias Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.13      0.18      2113\n",
      "           1       0.23      0.14      0.17      2267\n",
      "           2       0.10      0.01      0.01      2174\n",
      "           3       0.17      0.01      0.03      2224\n",
      "           4       0.83      0.72      0.77      1721\n",
      "           5       0.13      0.08      0.10      2222\n",
      "           6       0.13      0.01      0.03      2133\n",
      "           7       0.12      0.03      0.04      1649\n",
      "           8       0.47      0.58      0.52      1982\n",
      "           9       0.13      0.03      0.04      2113\n",
      "          10       0.12      0.05      0.07      1788\n",
      "          11       0.17      0.07      0.10      2081\n",
      "          12       0.13      0.09      0.11      2278\n",
      "          13       0.12      0.89      0.21      2161\n",
      "          14       0.54      0.63      0.58      1800\n",
      "\n",
      "    accuracy                           0.22     30706\n",
      "   macro avg       0.24      0.23      0.20     30706\n",
      "weighted avg       0.23      0.22      0.19     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.13076322 0.12763741 0.13288171 0.13079729 0.1378322  0.12949453\n",
      " 0.12272017 0.13574779 0.13288171 0.12662845]\n",
      "Straified cross validation scores: [0.13180516 0.12998177 0.13157895 0.126889   0.13288171 0.12897342\n",
      " 0.13548723 0.12819177 0.12819177 0.13340281]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_gaussian_naive_bias_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99a0c6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 5. Support Vector Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Best score found:  0.5385602924487439\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'kernel': 'rbf', 'gamma': 0.1, 'C': 1000}\n",
      "Best score found:  0.5385602924487439\n",
      "5. Support Vector Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.47      0.40      2113\n",
      "           1       0.56      0.57      0.56      2267\n",
      "           2       0.40      0.45      0.43      2174\n",
      "           3       0.56      0.68      0.61      2224\n",
      "           4       0.84      0.83      0.84      1721\n",
      "           5       0.45      0.43      0.44      2222\n",
      "           6       0.37      0.28      0.32      2133\n",
      "           7       0.43      0.34      0.38      1649\n",
      "           8       0.57      0.79      0.66      1982\n",
      "           9       0.80      0.83      0.81      2113\n",
      "          10       0.53      0.41      0.46      1788\n",
      "          11       0.35      0.24      0.29      2081\n",
      "          12       0.62      0.58      0.60      2278\n",
      "          13       0.77      0.71      0.74      2161\n",
      "          14       0.73      0.75      0.74      1800\n",
      "\n",
      "    accuracy                           0.56     30706\n",
      "   macro avg       0.56      0.56      0.55     30706\n",
      "weighted avg       0.55      0.56      0.55     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.26855952 0.27142485 0.28452319 0.27696717 0.27774883 0.27540386\n",
      " 0.27853048 0.28217822 0.27618551 0.26550287]\n",
      "Straified cross validation scores: [0.27142485 0.27142485 0.28295987 0.28191767 0.27696717 0.27774883\n",
      " 0.27488275 0.26628452 0.27331944 0.26862949]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_support_vector_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8578ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 6. K-Nearest Neighbors Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'metric': 'minkowski', 'n_neighbors': 100, 'p': 1, 'weights': 'distance'}\n",
      "Best score found:  0.5031243370554471\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'weights': 'distance', 'p': 1, 'n_neighbors': 100, 'metric': 'minkowski'}\n",
      "Best score found:  0.5031243370554471\n",
      "6. K-Nearest Neighbors metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.28      0.32      2113\n",
      "           1       0.58      0.49      0.53      2267\n",
      "           2       0.60      0.43      0.50      2174\n",
      "           3       0.47      0.74      0.57      2224\n",
      "           4       0.91      0.74      0.81      1721\n",
      "           5       0.34      0.45      0.39      2222\n",
      "           6       0.72      0.34      0.46      2133\n",
      "           7       0.48      0.25      0.33      1649\n",
      "           8       0.55      0.65      0.59      1982\n",
      "           9       0.66      0.84      0.74      2113\n",
      "          10       0.52      0.30      0.38      1788\n",
      "          11       0.38      0.16      0.23      2081\n",
      "          12       0.45      0.72      0.55      2278\n",
      "          13       0.60      0.61      0.61      2161\n",
      "          14       0.47      0.85      0.61      1800\n",
      "\n",
      "    accuracy                           0.52     30706\n",
      "   macro avg       0.54      0.52      0.51     30706\n",
      "weighted avg       0.54      0.52      0.51     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.36207346 0.35712425 0.373111   0.36425221 0.36868161 0.35617509\n",
      " 0.36842105 0.35982282 0.37623762 0.34445023]\n",
      "Straified cross validation scores: [0.3714509  0.35399844 0.36008338 0.37571652 0.36529442 0.36190724\n",
      " 0.3681605  0.36008338 0.36034393 0.3741532 ]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_knn_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5360ffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 7. Ada Boost Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n",
      "Best parameters found:  {'estimator': DecisionTreeClassifier(max_depth=3), 'learning_rate': 0.01, 'n_estimators': 100}\n",
      "Best score found:  0.3444527244086726\n",
      "\n",
      "==== Random Search: =====\n",
      "Best parameters found:  {'n_estimators': 100, 'learning_rate': 0.01, 'estimator': DecisionTreeClassifier(max_depth=3)}\n",
      "Best score found:  0.3444527244086726\n",
      "7. Ada Boost Classifier metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.62      0.37      2113\n",
      "           1       0.30      0.44      0.35      2267\n",
      "           2       0.57      0.03      0.06      2174\n",
      "           3       0.23      0.29      0.26      2224\n",
      "           4       0.00      0.00      0.00      1721\n",
      "           5       0.18      0.20      0.19      2222\n",
      "           6       0.23      0.00      0.00      2133\n",
      "           7       0.35      0.10      0.15      1649\n",
      "           8       0.38      0.76      0.51      1982\n",
      "           9       0.62      0.37      0.46      2113\n",
      "          10       0.23      0.04      0.06      1788\n",
      "          11       0.15      0.02      0.03      2081\n",
      "          12       0.22      0.63      0.33      2278\n",
      "          13       0.39      0.42      0.40      2161\n",
      "          14       0.64      0.75      0.69      1800\n",
      "\n",
      "    accuracy                           0.32     30706\n",
      "   macro avg       0.32      0.31      0.26     30706\n",
      "weighted avg       0.32      0.32      0.26     30706\n",
      "\n",
      "K-fold cross validaiton scores: [0.31388382 0.32352175 0.33090151 0.31605003 0.32595102 0.3238666\n",
      " 0.32647212 0.32490881 0.33324648 0.3238666 ]\n",
      "Straified cross validation scores: [0.32560563 0.32378223 0.3293382  0.31969776 0.32647212 0.32021886\n",
      " 0.3243877  0.31474726 0.33090151 0.31813445]\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_ada_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89859406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================== 8. XG Boost Classifier Section: ===================\n",
      "\n",
      "==== Grid Search: =====\n"
     ]
    }
   ],
   "source": [
    "w100_o25_pipeline.run_xg_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dea4b3",
   "metadata": {},
   "source": [
    "# == Model and scores for Window 100 & 50% Overlap =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76401191",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ModelEvaluationPipeline.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_evaluation_pipeline \u001b[38;5;241m=\u001b[39m ModelEvaluationPipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures/w100_o50_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow 100 & 50\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m Overlap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model_evaluation_pipeline\u001b[38;5;241m.\u001b[39mdriver()\n",
      "\u001b[0;31mTypeError\u001b[0m: ModelEvaluationPipeline.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "w100_o50_pipeline = ModelEvaluationPipeline(\"features/w100_o50_features.csv\", \"Window 100 & 50% Overlap\")\n",
    "w100_o50_pipeline.run_logistic_regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32154551",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_decission_tree_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ae675",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_random_forest_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01462d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_gaussian_naive_bias_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ed267",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_support_vector_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_knn_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_ada_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w100_o50_pipeline.run_xg_boost_classifier_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685b012",
   "metadata": {},
   "source": [
    "# == Model and scores for Window 200 & 25% Overlap =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w200_o25_features.csv\", \"Window 200 & 25% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w200_o50_features.csv\", \"Window 200 & 50% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w300_o25_features.csv\", \"Window 300 & 25% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdede7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w300_o50_features.csv\", \"Window 300 & 50% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w400_o25_features.csv\", \"Window 400 & 25% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w400_o50_features.csv\", \"Window 400 & 50% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w500_o25_features.csv\", \"Window 500 & 25% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62482853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_pipeline = ModelEvaluationPipeline(\"features/w500_o50_features.csv\", \"Window 500 & 50% Overlap\")\n",
    "model_evaluation_pipeline.driver()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlKernel",
   "language": "python",
   "name": "mlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
